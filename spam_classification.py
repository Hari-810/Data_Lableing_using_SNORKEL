# -*- coding: utf-8 -*-
"""Spam Classification.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Xk7pltZ82qQ2d6sXAqh-HboP7qAl9cNl
"""

!pip install snorkel

# import packages
import pandas as pd
import numpy as np
import re

#Load dataset

df_sms = pd.read_csv('https://raw.githubusercontent.com/tauseef1234/Datasets/main/spam.csv',encoding='ISO-8859-1')
df_sms.info()
df_sms.head()

# replacing nan with blank space
cols =['Unnamed: 2','Unnamed: 3','Unnamed: 4']
for col in cols:
  df_sms[col].replace(np.nan,'',regex=True, inplace=True)

# concatnating all columns
df_sms['v2']=df_sms['v2'] + df_sms['Unnamed: 2'].astype(str) + df_sms['Unnamed: 3'].astype(str) + df_sms['Unnamed: 4'].astype(str)

# drop the unnamed columns
df_sms.drop(columns=cols,inplace=True)

# rename fields
df_sms = df_sms.rename(columns={"v1": "label", "v2": "text"})
# shuffle order
df_sms = df_sms.sample(frac=1, random_state=123).reset_index(drop=True)

# split the data into train and test datasets
from sklearn.model_selection import train_test_split
df_train, df_test = train_test_split(
        df_sms, test_size=0.2, random_state=123, stratify=df_sms.label
    )

# display train and test data
print("Train")
display(df_train.head())
print('Test')
df_test.head()

# check train and test data dimension
print (df_train.info())
print (df_test.info())

df_train['label'].value_counts()

df_test['label'].value_counts()

# map constant values to the labels 'ham' and 'spam' in the dataset
dict_map = {'ham':HAM,'spam':SPAM}
Y_train = df_train['label'].map(dict_map).values
Y_test = df_test['label'].map(dict_map).values

# Defining Lables
ABSTAIN = -1
HAM = 0
SPAM = 1

# Creating Lableing functions
# Minimun 3 functions required

from snorkel.labeling import labeling_function

# Ham messages tend to have personal emoticons
@labeling_function()
def smiley_face(x):
    return HAM if re.search(r'(?::|;|=)(?:-)?(?:\)|\(|D|P)',x.text) else ABSTAIN

# SPAM messages can have post office as text
@labeling_function()
def regex_pobox(x):
    return SPAM if re.search(r"po.*box", x.text, flags=re.I) else ABSTAIN

# SPAM messages have various acronyms for terms and conditions
@labeling_function()
def regex_tc(x):
    return SPAM if re.search(r"T.{0,2}C", x.text) else ABSTAIN

# SPAM messages can have 5-digit text numbers or 10-digit phone numbers
@labeling_function()
def spam_numbers(x):
    return SPAM if re.search(r'\b\d{5}\b|\b\d{10}\b',x.text) else ABSTAIN

# HAM messages can have multiple full stops
@labeling_function()
def regex_punc(x):
    return HAM if re.search(r'\.{3,}',x.text) else ABSTAIN

from snorkel.labeling import LabelingFunction


def keyword_lookup(x, keywords, label):
    if any(word in x.text.lower() for word in keywords):
        return label
    return ABSTAIN

def make_keyword_lf(keywords, label=SPAM):
    return LabelingFunction(
        name=f"keyword_{keywords[0]}",
        f=keyword_lookup,
        resources=dict(keywords=keywords, label=label),
    )

"""Spam messages ask users to subscribe to their webpage or channel"""
keyword_subscribe = make_keyword_lf(keywords=["subscribe"])

"""Spam comments post links to webpage or channels"""
keyword_link = make_keyword_lf(keywords=["http","www"])

"""Spam comments make impersonal requests"""
keyword_please = make_keyword_lf(keywords=["urgent", "private","STOP", "satisfaction", "guaranteed", "FREE"])

"""Spam comments with calletunes and voicemail"""
keyword_song = make_keyword_lf(keywords=["callertunes","voicemail"])

"""Spam comments actually talk about win and award"""
keyword_win = make_keyword_lf(keywords=["reward", "points", "voucher", "bonus", "code", "gift", "claim", "prize", "expires", "award"])

"""HAM comments have personal references of family and friends"""
keyword_personal =make_keyword_lf(keywords=['bro','mum','papa', 'dad','daddy','mom','mommy', 'father', 'mother'],label=HAM)

"""HAM comments getting delivered via a third party website source"""
keyword_fullonsms =make_keyword_lf(keywords=['fullonsms.com'],label=HAM)

@labeling_function()
def short_comment(x):
    """Ham comments are often short, such as 'cool video!'"""
    return HAM if len(x.text.split()) < 5 else ABSTAIN

from snorkel.preprocess import preprocessor
from textblob import TextBlob
@preprocessor(memoize=True)
def textblob_sentiment(x):
    scores = TextBlob(x.text)
    x.polarity = scores.sentiment.polarity
    x.subjectivity = scores.sentiment.subjectivity
    return x 

@labeling_function(pre=[textblob_sentiment])
def textblob_polarity(x):
    return HAM if x.polarity > 0.9 else ABSTAIN

@labeling_function(pre=[textblob_sentiment])
def textblob_subjectivity(x):
    return HAM if x.subjectivity >= 0.5 else ABSTAIN

from snorkel.preprocess.nlp import SpacyPreprocessor

spacy = SpacyPreprocessor(text_field="text", doc_field="doc", memoize=True)


@labeling_function(pre=[spacy])
def has_person(x):
    """Ham comments mention specific people and are short."""
    if len(x.doc) < 20 and any([ent.label_ in ["PERSON","GPE"] for ent in x.doc.ents]):
        return HAM
    else:
        return ABSTAIN

from snorkel.labeling.lf.nlp import nlp_labeling_function
@nlp_labeling_function()
def has_person_nlp(x):
    """Ham comments mention specific people and are short."""
    if len(x.doc) < 20 and any([ent.label_ in ["PERSON","GPE"] for ent in x.doc.ents]):
        return HAM
    else:
        return ABSTAIN

# creating pipeline for lableing functions

lfs = [
    keyword_subscribe,
    keyword_link,
    keyword_win,
    regex_pobox,
    regex_tc,
    short_comment,
    keyword_please,
    keyword_personal,
    spam_numbers,
    keyword_fullonsms,
    regex_punc,
    smiley_face,
    has_person,
    textblob_polarity,
    textblob_subjectivity

]

from snorkel.labeling import PandasLFApplier
applier = PandasLFApplier(lfs=lfs)
L_train = applier.apply(df=df_train)
L_test = applier.apply(df=df_test)

# check some of those messages which the LF regex_tc labelled them as SPAM
pd.set_option('display.max_colwidth', None)
df_train.loc[L_train[:, 4] == SPAM].sample(20, random_state=3)

# Commented out IPython magic to ensure Python compatibility.
import matplotlib.pyplot as plt

# %matplotlib inline

def plot_label_frequency(L):
    plt.hist((L != ABSTAIN).sum(axis=1), density=True, bins=range(L.shape[1]))
    plt.xlabel("Number of labels")
    plt.ylabel("Fraction of dataset")
    plt.show()


plot_label_frequency(L_train)

from snorkel.labeling.model import MajorityLabelVoter

majority_model = MajorityLabelVoter()
preds_train = majority_model.predict(L=L_train)

from snorkel.labeling.model import LabelModel
label_model = LabelModel(cardinality=2,verbose=True) # cardinality is number of classes 2
label_model.fit(L_train,n_epochs=1000, log_freq=100,seed=3)

majority_acc = majority_model.score(L=L_test, Y=Y_test, tie_break_policy="random")[
    "accuracy"
]
print(f"{'Majority Vote Accuracy:':<25} {majority_acc * 100:.1f}%")

label_model_acc = label_model.score(L=L_test, Y=Y_test, tie_break_policy="random")[
    "accuracy"
]
print(f"{'Label Model Accuracy:':<25} {label_model_acc * 100:.1f}%")

probs_train = label_model.predict_proba(L=L_train)
probs_train

def plot_probabilities_histogram(Y):
  plt.hist(Y,bins=10)
  plt.xlabel("Probability of SPAM")
  plt.ylabel("Number of data points")
  plt.show()

plot_probabilities_histogram(probs_train[:,1])

from snorkel.labeling import filter_unlabeled_dataframe

df_train_filtered, probs_train_filtered = filter_unlabeled_dataframe(
    X=df_train, y=probs_train, L=L_train
)

probs_train.shape

print(f"{'train filtered:':<20}{df_train_filtered.shape}")
print(f"{'prob filtered:':<20}{probs_train_filtered.shape}")

from sklearn.feature_extraction.text import CountVectorizer

vectorizer = CountVectorizer(ngram_range=(1, 5))
X_train = vectorizer.fit_transform(df_train_filtered.text.tolist())
X_test = vectorizer.transform(df_test.text.tolist())

X_train.shape

from snorkel.utils import probs_to_preds

preds_train_filtered = probs_to_preds(probs=probs_train_filtered)

from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report, confusion_matrix

sklearn_model = LogisticRegression(C=1e3, solver="liblinear")
sklearn_model.fit(X=X_train, y=preds_train_filtered)

# checking accuracy
print(f"Test Accuracy: {sklearn_model.score(X=X_test, y=Y_test) * 100:.1f}%")

from sklearn import metrics

#predicted probablities for label 1
y_pred_prob = sklearn_model.predict_proba(X_test)[:,1]

fpr, tpr, thresholds = metrics.roc_curve(Y_test,y_pred_prob,pos_label=1)
plt.plot(fpr, tpr, 'b')
plt.plot([0,1],[0,1], 'r--')
plt.xlabel("False Positive Rate", fontsize=12)
plt.ylabel("True Positive Rate", fontsize=12)

metrics.auc(fpr, tpr)

# storing predicted binary values by the model
pred = sklearn_model.predict(X_test)

# check classiification metrics
print(classification_report(Y_test, pred))

# plot the confusion matrix
cm = confusion_matrix(Y_test, pred)

plt.rcParams['xtick.top'] = plt.rcParams['xtick.labeltop'] = True
plt.rcParams['xtick.bottom'] = plt.rcParams['xtick.labelbottom'] = False
fig, ax = plt.subplots(figsize=(4, 3))
ax.imshow(cm)
ax.grid(False)
ax.xaxis.set(ticks=(0, 1), ticklabels=('Predicted 0s', 'Predicted 1s'))
ax.yaxis.set(ticks=(0, 1), ticklabels=('Actual 0s', 'Actual 1s'))
ax.set_ylim(1.5, -0.5)

for i in range(2):
    for j in range(2):
        ax.text(j, i, cm[i, j], ha='center', va='center', color='red')
plt.show()